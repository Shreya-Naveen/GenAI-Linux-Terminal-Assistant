# -*- coding: utf-8 -*-
"""GenAI_CLI_Terminal_Assistant

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-FAUM6uyr6Szwun9Wfsx92ejiiQhmuGq

# **Steps to execute:**
1. Don't run the code for extracting man pages, extracting chunks and embeddings and storing in chromadb.
2. The chromadb folder sent must be saved in drive/MyDrive.
3. The code can be run continuously from the rag framework part till the agentic part incorporating text input is finished. The test cases are mentioned in the output of the agentic workflow tested earlier.
4. For testing out speech input execution, in your /content folder, an empty audio file named user_audio.wav must be created before executing.
5. Based on what agent action(main, fix, autocomplete, etc.) you want to perform, the cell recording speech and converting to text must be modified accordingly.
6. Fine tuning of gpt2 and evaluation of fine tuned model using bleu, rouge, bert score can be run subsequently.
"""

'''running this code is not needed as it is for extracting linux man pages which will later be stored in the chromadb'''


import requests
from bs4 import BeautifulSoup

BASE_URL = "https://man7.org/linux/man-pages/"

def get_commands_from_section(section):
    """Extracts command names from a specific man section"""
    section_url = f"{BASE_URL}{section}/"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    response = requests.get(section_url, headers=headers)
    if response.status_code != 200:
        print(f"Failed to fetch section {section}. Status Code: {response.status_code}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    commands = []

    for link in soup.find_all("a", href=True):
        if link["href"].endswith(".html"):  # Valid command pages
            command_name = link.text.strip()
            command_url = BASE_URL + section + "/" + link["href"]
            commands.append((command_name, command_url))

    return commands

def get_all_linux_commands():
    """Extracts commands from all major sections"""
    sections = ["man1", "man2", "man3", "man4", "man5", "man6", "man7", "man8"]  # Common sections
    all_commands = []

    for section in sections:
        print(f"Fetching commands from {section}...")
        commands = get_commands_from_section(section)
        all_commands.extend(commands)

    return all_commands

# Run the function
linux_commands = get_all_linux_commands()

# Print results
print(f"\nTotal commands extracted: {len(linux_commands)}")
for cmd, url in linux_commands[:10]:  # Show first 10
    print(f"{cmd} -> {url}")

!pip install flagEmbedding chromadb nltk tiktoken #running this code for all the libraries to be downloaded

'''running this code is not necessary as the model used here (bge-m3) is used for generating embeddings for the man pages'''

from FlagEmbedding import BGEM3FlagModel

# Load the BGE-M3 embedding model (FP16 for performance boost)
embedding_model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True)

def generate_embedding(text):
    """
    Generates an embedding for the given text chunk.

    Args:
        text (str): The text chunk to embed.

    Returns:
        list: List of embedding values.
    """
    # Ensure input is a list (required by encode method)
    if isinstance(text, str):
        text = [text]

    # Generate embeddings
    embeddings = embedding_model.encode(text, max_length=8192)  # Ensures large text compatibility

    # Extract dense embeddings correctly
    return embeddings["dense_vecs"][0]  # Returns first embedding as a list

!pip install -U langchain_google_genai #running code for downloading necessary libraries

''' this code is using the bge-m3 embedding model to extract linux commands from man pages manually, splitting the texts into chunks, embedding them and then storing in chroma db.
We've already extracted 2 man pages and couldn't extract more due to resource and time constraints and hence will send over the chromadb file to be used directly as a vector database in the rag and agentic workflow'''

import requests
from bs4 import BeautifulSoup
import chromadb
import shutil
from FlagEmbedding import BGEM3FlagModel  # Import embedding model

BASE_URL = "https://man7.org/linux/man-pages/"

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

CHROMA_DB_PATH = "/content/drive/MyDrive/chroma_db"  # Original database path
LOCAL_DB_PATH = "/content/chroma_db"  # Temporary writable copy

# Copy database to a writable location
shutil.copytree(CHROMA_DB_PATH, LOCAL_DB_PATH, dirs_exist_ok=True)

chroma_client = chromadb.PersistentClient(path=LOCAL_DB_PATH)
collection = chroma_client.get_or_create_collection(name="linux_manuals")

# Load Embedding Model
embedding_model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True)

def get_commands_with_manuals():
    """Extracts Linux command names and retrieves their manual texts from all 8 man pages."""
    sections = [f"man{i}" for i in range(1, 3)]  # ['man1', 'man2']
    linux_manuals = {}

    for section in sections:
        print(f" Fetching commands from {section}...")
        commands = get_commands_from_section(section)

        for command_name, command_url in commands:
            manual_text = fetch_manual_text(command_url)
            if manual_text:  # Only store if text is extracted
                linux_manuals[command_name] = manual_text

        print(f" {section} done")

    return linux_manuals

def fetch_manual_text(command_url):
    """Fetches and extracts text from a Linux man page."""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    try:
        response = requests.get(command_url, headers=headers, timeout=10)
        response.raise_for_status()
    except requests.RequestException:
        return None  # Fail silently

    soup = BeautifulSoup(response.text, "html.parser")
    manual_text = soup.get_text(separator=" ", strip=True)  # Extract readable text
    return manual_text if manual_text.strip() else None

def chunk_text(text, chunk_size=512):
    """Splits text into smaller chunks for embedding."""
    words = text.split()
    return [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

def generate_embedding(text):
    """Generates an embedding for the given text chunk."""
    embedding_output = embedding_model.encode(text)
    if isinstance(embedding_output, dict) and "dense_vecs" in embedding_output:
        return embedding_output["dense_vecs"].tolist()  # Convert NumPy array to list
    raise ValueError("Unexpected embedding output format from BGEM3FlagModel.")

def store_in_chromadb(command, chunks):
    """Embeds and stores command manual chunks in ChromaDB."""
    for i, chunk in enumerate(chunks):
        embedding = generate_embedding(chunk)  # Convert chunk into vector
        collection.add(
            documents=[chunk],
            embeddings=[embedding],  # Store embedding
            metadatas=[{"command": command}],
            ids=[f"{command}_{i}"]
        )
    print(f" Stored {len(chunks)} chunks for '{command}' in ChromaDB.")

# Extract and store manuals
linux_manuals = get_commands_with_manuals()

for command, manual_text in linux_manuals.items():
    chunks = chunk_text(manual_text)  # Step 1: Chunking
    store_in_chromadb(command, chunks)  # Step 2 & 3: Embedding + Storing

# Copy the updated database back to Google Drive
shutil.copytree(LOCAL_DB_PATH, CHROMA_DB_PATH, dirs_exist_ok=True)

print(" Linux manuals stored successfully in ChromaDB with embeddings!")'''

from google.colab import drive
drive.mount('/content/drive')

import os
import chromadb

chromadb_path = "/content/drive/MyDrive/chroma_db"
files = os.listdir(chromadb_path)

print("Files in /content/drive/MyDrive/chroma_db:")
for file in files:
    print(file)

import chromadb

CHROMA_DB_PATH = "/content/drive/MyDrive/chroma_db"
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

collections = chroma_client.list_collections()
print("Available collections:", [col for col in collections])  # Directly print collection names

collection = chroma_client.get_collection(name="linux_manuals")

# Get a sample of stored documents
documents = collection.peek()  # Retrieves a few stored items
print("Sample documents:", documents)

'''this code is only a rag framework that extracts relevant top 3 linux commands from the chromadb database based on the user query'''

import chromadb

def search_command(query, top_k=3):
    """Searches for the most relevant Linux manual sections based on the query."""

    # Initialize ChromaDB client
    chroma_client = chromadb.PersistentClient(path="/content/drive/MyDrive/chroma_db")
    collection = chroma_client.get_or_create_collection(name="linux_manuals")

    # Generate embedding for the user query
    query_embedding = generate_embedding(query)  # Use the same embedding function

    # Perform similarity search in ChromaDB
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k  # Retrieve top-k most relevant results
    )

    # Check if results exist
    if not results["documents"]:
        return " No relevant manual found."

    # Format and return results
    response = " **Relevant Linux Commands:**\n\n"
    for i, doc in enumerate(results["documents"][0]):
        command = results["metadatas"][0][i]["command"]
        response += f"🔹 **{command}**\n{doc[:500]}...\n\n"  # Display first 500 chars

    return response

# Example usage
user_query = "How do I list files in Linux?"
print(search_command(user_query))

query_text = "What does the 'as' command do in Linux?"
query_embedding = embedding_model.encode(query_text)["dense_vecs"].tolist()

results = collection.query(
    query_embeddings=[query_embedding],
    n_results=5  # Adjust as needed
)

print("Search results:", results)

# Check document count
doc_count = collection.count()
print(f"📂 Total documents in collection: {doc_count}")

# Fetch a few stored documents
sample_docs = collection.get(limit=3)
print("📄 Sample Documents:", sample_docs)

!pip install -U langchain langchain-community google-generativeai langchain_google_genai #relevant libraries downloaded

!chmod -R 777 /content/drive/MyDrive/chroma_db

from langchain_community.llms import HuggingFaceHub
from langchain_community.embeddings import HuggingFaceEmbeddings
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

import google.generativeai as genai

genai.configure(api_key="AIzaSyBD5dECyML_lefRHqLf6BPmV3h7AtA2YkQ")

"""final code for the agentic workflow to be run"""

import os
from langchain.agents import initialize_agent, Tool
from langchain.agents.agent_types import AgentType
from langchain.memory import ConversationBufferMemory
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.vectorstores import Chroma
from langchain_core.tools import tool
from langchain_google_genai import ChatGoogleGenerativeAI
from termcolor import colored
from rich import print
import subprocess

os.environ["GOOGLE_API_KEY"] =

# Gemini LLM
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0,
    google_api_key=os.environ["GOOGLE_API_KEY"]
)

#  Vector DB + Retriever
embedding = HuggingFaceBgeEmbeddings(model_name="BAAI/bge-m3", model_kwargs={"device": "cuda"})
db = Chroma(persist_directory="linux_manuals_chroma", embedding_function=embedding)
retriever = db.as_retriever()

#  Tool 1: Search Linux Manual (RAG-based)
@tool
def search_linux_manual(query: str) -> str:
    """Search Linux manual and provide a helpful answer using retrieved documents."""
    docs = retriever.get_relevant_documents(query)
    context = "\n\n".join(doc.page_content for doc in docs[:5])
    prompt = f"""You are a thoughtful Linux assistant. Let’s think step-by-step.

1. Understand the user's query.
2. Search relevant manual entries.
3. Analyze and summarize key concepts.
4. Provide a well-reasoned and helpful answer.

User Query: {query}

Context:
{context}

Answer:"""
    return llm.invoke(prompt).content

# Tool 2: Fix Bash Commands
@tool
def fix_bash_command(command: str) -> str:
    """Fix, correct, or improve a given Linux bash command."""
    prompt = f"""You're a bash expert. Let’s think step-by-step.

1. Identify issues with the command.
2. Suggest a corrected version.
3. Explain why the fix works.

Command: {command}
"""
    return llm.invoke(prompt).content

# Tool 3: ASCII Diagram Generator
@tool
def generate_ascii_diagram(topic: str) -> str:
    """Generate an ASCII-style explanation or diagram from a Linux concept."""
    prompt = f"""You're a Linux tutor. Let’s think step-by-step.

1. Identify what this Linux concept is.
2. Break it down visually or structurally.
3. Generate a simple ASCII explanation or diagram.

Topic: {topic}
"""
    return llm.invoke(prompt).content

# Tool 4: Autocomplete Command
@tool
def autocomplete_command(fragment: str) -> str:
    """Suggest common completions or full commands based on the input."""
    docs = retriever.get_relevant_documents(fragment)
    examples = "\n".join(doc.page_content for doc in docs[:3])
    prompt = f"""You are a Linux command completion assistant. Let’s think step-by-step.

1. Interpret what the user is trying to do.
2. Use common command patterns.
3. Suggest a complete and valid command.

User typed: `{fragment}`

Reference Commands:
{examples}

Your Suggestions:"""
    return llm.invoke(prompt).content

#  Tool 5: Disk Analyzer
@tool(description="Analyzes disk space and returns a summary of disk usage.")
def analyze_disk_space_func() -> str:
    try:
        output = subprocess.check_output(["df", "-h"], text=True)
        return (
            "Let’s think step-by-step:\n"
            "1. We’ll use the `df -h` command to view disk space.\n"
            "2. It shows mounted partitions and their usage.\n"
            "3. Here's the result:\n\n" + output
        )
    except subprocess.CalledProcessError as e:
        return f"❌ Failed to analyze disk space: {str(e)}"

analyze_disk_space = Tool(
    name="analyze_disk_space",
    func=analyze_disk_space_func,
    description="Analyzes disk space and returns a summary of disk usage"
)

# Memory
memory = ConversationBufferMemory(memory_key="chat_history")

# Agents
main_agent = initialize_agent(
    tools=[search_linux_manual],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    memory=memory
)

fix_agent = initialize_agent(
    tools=[fix_bash_command],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

autocomplete_agent = initialize_agent(
    tools=[autocomplete_command],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

disk_agent = initialize_agent(
    tools=[analyze_disk_space],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    memory=memory
)

ascii_agent = initialize_agent(
    tools=[generate_ascii_diagram],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# Agent Router with Chain-of-Thought Prompts
def run_assistant(user_query: str, agent_type: str = "main"):
    print(colored(f"🧠 Query: {user_query}", "cyan"))
    cot_prompt = f"Let’s think step-by-step about this task:\n{user_query}"

    if agent_type == "fix":
        return fix_agent.run(cot_prompt)
    elif agent_type == "autocomplete":
        return autocomplete_agent.run(cot_prompt)
    elif agent_type == "disk":
        return disk_agent.run("I want to understand the disk usage step-by-step.")
    elif agent_type == "ascii":
        return ascii_agent.run(cot_prompt)
    else:
        return main_agent.run(cot_prompt)

# CLI Loop
if __name__ == "__main__":
    print("🔧 Linux Assistant is ready. Type 'quit' or 'exit' to end.")

    while True:
        mode = input("Choose agent [main / fix / autocomplete / disk / ascii]: ").lower()

        if mode in ["quit", "exit"]:
            print("👋 Goodbye!")
            break

        if mode not in ["main", "fix", "autocomplete", "disk", "ascii"]:
            print(colored("Invalid agent selection. Please choose from [main, fix, autocomplete, disk, ascii].", "red"))
            continue

        if mode == "disk":
            response = run_assistant("Check disk usage", agent_type=mode)
            print("\n📊 Disk Info:\n", response)
            continue

        query = input("\nEnter your query: ")

        if query.lower() in ["quit", "exit"]:
            print("👋 Goodbye!")
            break

        response = run_assistant(query, mode)
        print(colored(f"🔧 Response:\n{response}", "green"))

'''the above code was for text, the following codes are for audio input processing'''
#downloading important libraries
!pip install -q openai-whisper
!pip install -q pydub
!sudo apt-get install -y ffmpeg

'''it is required to create an initial empty user_audio.wav file in the content/ folder before running the below code'''
'''the audio recorded here is only for 8 seconds which can be increased according to convenience'''

from IPython.display import display, Javascript
from google.colab import output
import base64
import time

# JavaScript to record audio from mic for 4 seconds
RECORD = """
const sleep = time => new Promise(resolve => setTimeout(resolve, time))
const b2text = blob => new Promise(resolve => {
  const reader = new FileReader();
  reader.onloadend = () => resolve(reader.result.split(',')[1]);
  reader.readAsDataURL(blob);
});

var record = async () => {
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  const mediaRecorder = new MediaRecorder(stream);
  const audioChunks = [];

  mediaRecorder.ondataavailable = event => {
    if (event.data.size > 0) audioChunks.push(event.data);
  };

  mediaRecorder.start();
  await sleep(8000);  // Record for 8 seconds, can be increased according to convenience
  mediaRecorder.stop();

  await new Promise(resolve => mediaRecorder.onstop = resolve);
  const audioBlob = new Blob(audioChunks);
  const base64Audio = await b2text(audioBlob);
  return base64Audio;
};

record().then(audio => {
  google.colab.kernel.invokeFunction('notebook.save_audio', [audio], {});
});
"""

# Callback to save the audio in user_audio.wav
def save_audio(audio_base64):
    audio_bytes = base64.b64decode(audio_base64)
    with open("user_audio.wav", "wb") as f:

        f.write(audio_bytes)
    print("🎙️ Audio saved as 'user_audio.wav'")

# Register the callback
output.register_callback("notebook.save_audio", save_audio)


# Display recorder
display(Javascript(RECORD))

# Wait for audio to be recorded and saved
time.sleep(15)

import whisper

# Load whisper model
model = whisper.load_model("base")

# Transcribe the saved audio
result = model.transcribe("user_audio.wav")
query = result["text"]

print("🗣️ You said:", query) #converts speech to text

# Assuming you already defined run_assistant() and tools as in your original code , now we can run our initial workflow with speech input given in above code cells
response = run_assistant(query, agent_type="main")
print("🤖 Assistant Response:\n", response)

#other inputs can be used based on the type of agents you want to use
# Assuming you already defined run_assistant() and tools as in your original code
response = run_assistant(query, agent_type="fix")
print("🤖 Assistant Response:\n", response)

#other inputs can be used based on the type of agents you want to use. for eg , to use autocomplete agent , speech input can be given something as : git
# Assuming you already defined run_assistant() and tools as in your original code
response = run_assistant(query, agent_type="autocomplete")
print("🤖 Assistant Response:\n", response)

#voice input given here can be disk analysis , for different voice inputs the above code cell recording audio and transcribing it should be run
# Assuming you already defined run_assistant() and tools as in your original code
response = run_assistant(query, agent_type="disk")
print("🤖 Assistant Response:\n", response)

!pip install peft==0.3.0 transformers==4.28.1 accelerate bitsandbytes datasets trl langchain_google_genai langchain_community langchain google-generativeai termcolor rich

!pip install -U transformers # update the transformers library
!pip install -U peft # update the peft library

'''since geminiai cant be publicly finetuned, we decided to use gpt2 and finetune it as part of comparative study'''
!pip install transformers peft chromadb

import os
import torch
import chromadb
from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from peft import get_peft_model, LoraConfig
from torch.utils.data import Dataset

# === Connect to ChromaDB and Load Manpages ===
CHROMA_PATH = "/content/drive/MyDrive/chroma_db"
chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)
collection = chroma_client.get_collection(name="linux_manuals")

documents = collection.get()
texts = documents["documents"][:500]
commands = [meta["command"] for meta in documents["metadatas"][:500]]


# === Prepare Paired Training Data ===
train_pairs = [{"input": cmd, "output": text} for cmd, text in zip(commands, texts)]

# === Load Tokenizer & Model ===
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # Fix for padding
model = GPT2LMHeadModel.from_pretrained("gpt2")

# === Apply LoRA ===
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# === Custom Dataset Class ===
class ManpageDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=512):
        self.examples = []
        for pair in data:
            text = f"{pair['input']}\n{pair['output']}"
            tokens = tokenizer(text, truncation=True, padding="max_length", max_length=max_length, return_tensors="pt")
            tokens["labels"] = tokens["input_ids"].clone()
            self.examples.append(tokens)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        return {k: v.squeeze(0) for k, v in self.examples[idx].items()}

# === Prepare Dataset ===
train_dataset = ManpageDataset(train_pairs, tokenizer)

# === Define Training Arguments ===
training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/fine_tuned_linux_model",
    overwrite_output_dir=True,
    per_device_train_batch_size=4,
    num_train_epochs=3,
    logging_steps=10,
    save_steps=500,
    save_total_limit=2,
    #evaluation_strategy="no",
    fp16=True  # Optional: enable for faster training with supported GPU
)

# === Trainer Setup ===
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

# === Train the Model ===
trainer.train()

# === Save Fine-tuned Model ===
model.save_pretrained("/content/drive/MyDrive/fine_tuned_linux_model")
tokenizer.save_pretrained("/content/drive/MyDrive/fine_tuned_linux_model")

!pip install --upgrade --force-reinstall torchvision

#evaluation of the fine tuned model done
import json

# Create the evaluation data in JSON format
evaluation_data = [
    {
        "prompt": "What does the `ls` command do?",
        "expected_response": "The `ls` command lists directory contents."
    },
    {
        "prompt": "Fix the command: cd..",
        "expected_response": "The correct command is: cd .."
    },
    {
        "prompt": "Generate an ASCII diagram of a Linux file structure.",
        "expected_response": "/ (root)\n├── bin\n├── etc\n├── home\n│   └── user\n└── var"
    }
]

# Save the data to a JSON file
with open("/content/evaluation_data.json", "w") as f:
    json.dump(evaluation_data, f, indent=4)

print("Evaluation data saved to 'evaluation_data.json'")

pip install transformers datasets evaluate torch

!pip install rouge_score
!pip install bert_score

#finetuning evaluation using bleu, rouge, bert score
import json
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import evaluate

# === Load Evaluation Dataset ===
with open("evaluation_data.json", "r") as f:
    eval_data = json.load(f)

# === Evaluation Functions ===
def get_responses(pipeline_fn, eval_data):
    preds, refs = [], []
    for item in eval_data:
        prompt = item["prompt"]
        expected = item["expected_response"]

        # Use the pipeline to generate responses
        output = pipeline_fn(prompt)[0]["generated_text"]
        preds.append(output)
        refs.append(expected)
    return preds, refs

# === Load Fine-Tuned Model ===
FT_MODEL_PATH = "/content/drive/MyDrive/fine_tuned_linux_model"  # Update with your fine-tuned model path

tokenizer = AutoTokenizer.from_pretrained(FT_MODEL_PATH, local_files_only=True)

# Fine-tuned pipeline (use AutoModelForCausalLM for fine-tuned GPT-style models)
ft_model = AutoModelForCausalLM.from_pretrained(FT_MODEL_PATH, device_map="auto", torch_dtype="auto", local_files_only=True)
ft_pipe = pipeline("text-generation", model=ft_model, tokenizer=tokenizer, max_new_tokens=256)

# === Generate Fine-Tuned Model Outputs ===
print("🔍 Generating fine-tuned model outputs...")
ft_preds, ft_refs = get_responses(ft_pipe, eval_data)

# === Evaluate Fine-Tuned Model ===
bleu = evaluate.load("bleu")
rouge = evaluate.load("rouge")
bertscore = evaluate.load("bertscore")

# Evaluate BLEU score for the fine-tuned model
print("\n📊 BLEU Score")
print("Fine-Tuned:", bleu.compute(predictions=ft_preds, references=ft_refs))

# Evaluate ROUGE score for the fine-tuned model
print("\n📊 ROUGE Score")
print("Fine-Tuned:", rouge.compute(predictions=ft_preds, references=ft_refs))

# Evaluate BERTScore for the fine-tuned model
print("\n📊 BERTScore")
print("Fine-Tuned:", bertscore.compute(predictions=ft_preds, references=ft_refs, lang="en"))